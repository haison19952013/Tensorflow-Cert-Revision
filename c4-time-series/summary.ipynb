{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary for the fourth chapter of the specialization course \"DeepLearning.AI TensorFlow Developer Professional Certificate\" on Coursera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic imports\n",
    "```python\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "print(tf.__version__)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Naive forcast\n",
    "```python\n",
    "naive_forecast = SERIES[SPLIT_TIME - 1:-1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Moving average\n",
    "```python\n",
    "def moving_average_forecast(series, window_size):\n",
    "    \"\"\"Forecasts the mean of the last few values.\n",
    "        If window_size=1, then this is equivalent to naive forecast\"\"\"\n",
    "    \n",
    "    forecast = []\n",
    "    \n",
    "    ### START CODE HERE  \n",
    "    for time in range(len(series) - window_size):\n",
    "        forecast.append(series[time:time + window_size].mean())\n",
    "        \n",
    "    # Conver to a numpy array\n",
    "    np_forecast = np.array(forecast)\n",
    "    \n",
    "    ### END CODE HERE\n",
    "    \n",
    "    return np_forecast\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Window dataset\n",
    "```python\n",
    "def windowed_dataset(series, window_size=G.WINDOW_SIZE, batch_size=G.BATCH_SIZE, shuffle_buffer=G.SHUFFLE_BUFFER_SIZE):\n",
    "    \n",
    "    ### START CODE HERE\n",
    "    \n",
    "    # Create dataset from the series\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "    \n",
    "    # Slice the dataset into the appropriate windows\n",
    "    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "    \n",
    "    # Flatten the dataset\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
    "    \n",
    "    # Shuffle it\n",
    "    dataset = dataset.shuffle(shuffle_buffer)\n",
    "    \n",
    "    # Split it into the features and labels\n",
    "    dataset = dataset.map(lambda window: (window[:-1], window[-1]))\n",
    "    \n",
    "    # Batch it\n",
    "    dataset = dataset.batch(batch_size).prefetch(1)\n",
    "    \n",
    "    ### END CODE HERE\n",
    "    \n",
    "    return dataset\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Generate the forecast\n",
    "```python\n",
    "def generate_forecast(series=G.SERIES, split_time=G.SPLIT_TIME, window_size=G.WINDOW_SIZE):\n",
    "    forecast = []\n",
    "    for time in range(len(series) - window_size):\n",
    "        forecast.append(model.predict(series[time:time + window_size][np.newaxis]))\n",
    "\n",
    "    forecast = forecast[split_time-window_size:]\n",
    "    results = np.array(forecast)[:, 0, 0]\n",
    "    return results\n",
    "\n",
    "\n",
    "# Save the forecast\n",
    "dnn_forecast = generate_forecast()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Use RNN\n",
    "```python\n",
    "def create_uncompiled_model():\n",
    "\n",
    "    ### START CODE HERE\n",
    "    \n",
    "    model = tf.keras.models.Sequential([ \n",
    "        tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),\n",
    "                      input_shape=[G.WINDOW_SIZE]),\n",
    "        tf.keras.layers.SimpleRNN(40, return_sequences=True),\n",
    "        tf.keras.layers.SimpleRNN(40),\n",
    "        tf.keras.layers.Dense(1),\n",
    "        tf.keras.layers.Lambda(lambda x: x * 100.0)\n",
    "    ]) \n",
    "    \n",
    "    ### END CODE HERE\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_uncompiled_model():\n",
    "\n",
    "    ### START CODE HERE\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "          tf.keras.layers.Conv1D(filters=60, kernel_size=5,\n",
    "                              strides=1, padding=\"causal\",\n",
    "                              activation=\"relu\",\n",
    "                              input_shape=[None, 1]),\n",
    "          tf.keras.layers.LSTM(60, return_sequences=True),\n",
    "          tf.keras.layers.LSTM(60, return_sequences=False),\n",
    "          tf.keras.layers.Dense(30, activation=\"relu\"),\n",
    "          tf.keras.layers.Dense(10, activation=\"relu\"),\n",
    "          tf.keras.layers.Dense(1),\n",
    "          tf.keras.layers.Lambda(lambda x: x * 400)\n",
    "        ])\n",
    "    \n",
    "    ### END CODE HERE\n",
    "\n",
    "    return model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Adjusting the learning rate\n",
    "```python\n",
    "def adjust_learning_rate():\n",
    "    \n",
    "    model = create_uncompiled_model()\n",
    "    \n",
    "    lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-6 * 10**(epoch / 20))\n",
    "    \n",
    "    ### START CODE HERE\n",
    "    \n",
    "    # Select your optimizer\n",
    "    optimizer = tf.keras.optimizers.SGD(momentum=0.9)\n",
    "    \n",
    "    # Compile the model passing in the appropriate loss\n",
    "    model.compile(loss=tf.keras.losses.Huber(),\n",
    "                  optimizer=optimizer, \n",
    "                  metrics=[\"mae\"]) \n",
    "    \n",
    "    ### END CODE HERE\n",
    "    \n",
    "    history = model.fit(dataset, epochs=100, callbacks=[lr_schedule])\n",
    "    \n",
    "    return history\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Fast forecasting\n",
    "```python\n",
    "def model_forecast(model, series, window_size):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    ds = ds.window(window_size, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda w: w.batch(window_size))\n",
    "    ds = ds.batch(32).prefetch(1)\n",
    "    forecast = model.predict(ds)\n",
    "    return forecast\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Parsing the raw data\n",
    "```python   \n",
    "def parse_data_from_file(filename):\n",
    "    \n",
    "    times = []\n",
    "    temperatures = []\n",
    "\n",
    "    with open(filename) as csvfile:\n",
    "        \n",
    "        ### START CODE HERE\n",
    "        \n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        next(reader)\n",
    "        step=0\n",
    "        for row in reader:\n",
    "            temperatures.append(float(row[1]))\n",
    "            times.append(step)\n",
    "            step = step + 1\n",
    "        ### END CODE HERE\n",
    "            \n",
    "    return times, temperatures\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Model prediction\n",
    "```python\n",
    "## NOTE: If you are using Safari and this cell throws an error,\n",
    "## please skip this block and run the next one instead.\n",
    "\n",
    "import numpy as np\n",
    "from google.colab import files\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    " \n",
    "  # predicting images\n",
    "  path = '/content/' + fn\n",
    "  img = load_img(path, target_size=(300, 300))\n",
    "  x = img_to_array(img)\n",
    "  x /= 255\n",
    "  x = np.expand_dims(x, axis=0)\n",
    "\n",
    "  images = np.vstack([x])\n",
    "  classes = model.predict(images, batch_size=10)\n",
    "  print(classes[0])\n",
    "    \n",
    "  if classes[0]>0.5:\n",
    "    print(fn + \" is a human\")\n",
    "  else:\n",
    "    print(fn + \" is a horse\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Visualizing Intermediate Representations\n",
    "```python\n",
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.keras.utils import img_to_array, load_img\n",
    "\n",
    "# Define a new Model that will take an image as input, and will output\n",
    "# intermediate representations for all layers in the previous model after\n",
    "# the first.\n",
    "successive_outputs = [layer.output for layer in model.layers[1:]]\n",
    "visualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n",
    "\n",
    "# Prepare a random input image from the training set.\n",
    "horse_img_files = [os.path.join(train_horse_dir, f) for f in train_horse_names]\n",
    "human_img_files = [os.path.join(train_human_dir, f) for f in train_human_names]\n",
    "img_path = random.choice(horse_img_files + human_img_files)\n",
    "\n",
    "img = load_img(img_path, target_size=(300, 300))  # this is a PIL image\n",
    "x = img_to_array(img)  # Numpy array with shape (300, 300, 3)\n",
    "x = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 300, 300, 3)\n",
    "\n",
    "# Scale by 1/255\n",
    "x /= 255\n",
    "\n",
    "# Run the image through the network, thus obtaining all\n",
    "# intermediate representations for this image.\n",
    "successive_feature_maps = visualization_model.predict(x)\n",
    "\n",
    "# These are the names of the layers, so you can have them as part of the plot\n",
    "layer_names = [layer.name for layer in model.layers[1:]]\n",
    "\n",
    "# Display the representations\n",
    "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
    "  if len(feature_map.shape) == 4:\n",
    "\n",
    "    # Just do this for the conv / maxpool layers, not the fully-connected layers\n",
    "    n_features = feature_map.shape[-1]  # number of features in feature map\n",
    "\n",
    "    # The feature map has shape (1, size, size, n_features)\n",
    "    size = feature_map.shape[1]\n",
    "    \n",
    "    # Tile the images in this matrix\n",
    "    display_grid = np.zeros((size, size * n_features))\n",
    "    for i in range(n_features):\n",
    "      x = feature_map[0, :, :, i]\n",
    "      x -= x.mean()\n",
    "      x /= x.std()\n",
    "      x *= 64\n",
    "      x += 128\n",
    "      x = np.clip(x, 0, 255).astype('uint8')\n",
    "    \n",
    "      # Tile each filter into this big horizontal grid\n",
    "      display_grid[:, i * size : (i + 1) * size] = x\n",
    "    \n",
    "    # Display the grid\n",
    "    scale = 20. / n_features\n",
    "    plt.figure(figsize=(scale * n_features, scale))\n",
    "    plt.title(layer_name)\n",
    "    plt.grid(False)\n",
    "    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1. Callbaks: [tf.keras.callbacks.Callback](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
